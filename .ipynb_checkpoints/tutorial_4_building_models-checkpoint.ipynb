{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e6c423",
   "metadata": {},
   "source": [
    "# Building Models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01fb4c",
   "metadata": {},
   "source": [
    "torch.nn.Module and torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6209aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model: \n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer: \n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model parameters: \n",
      "Parameter containing:\n",
      "tensor([[-6.8152e-02, -4.9489e-02, -8.2287e-02,  ...,  2.8960e-02,\n",
      "          1.4670e-02,  8.8196e-02],\n",
      "        [-2.8234e-02, -6.3169e-05, -3.1043e-02,  ..., -9.3712e-02,\n",
      "          5.0704e-02,  6.6511e-02],\n",
      "        [-6.9824e-02,  8.2345e-02,  3.6071e-02,  ..., -3.8940e-02,\n",
      "         -7.9457e-02,  6.8927e-02],\n",
      "        ...,\n",
      "        [ 4.5794e-02,  1.1396e-02,  4.6661e-02,  ...,  3.1534e-02,\n",
      "         -4.7502e-02,  6.9681e-02],\n",
      "        [-2.2402e-02, -6.4405e-02, -1.8062e-02,  ..., -7.8268e-02,\n",
      "          9.5082e-02,  6.2663e-03],\n",
      "        [-4.8274e-02,  1.5618e-02, -9.9394e-02,  ...,  5.3213e-02,\n",
      "          8.4867e-02, -3.6768e-04]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0470,  0.0928, -0.0745, -0.0330,  0.0918,  0.0362,  0.0163,  0.0272,\n",
      "         0.0862,  0.0257, -0.0130, -0.0268, -0.0870,  0.0732, -0.0247, -0.0492,\n",
      "         0.0150,  0.0170,  0.0272, -0.0653, -0.0327,  0.0791, -0.0601,  0.0545,\n",
      "         0.0211, -0.0419, -0.0111,  0.0073, -0.0528, -0.0095, -0.0190,  0.0777,\n",
      "         0.0185, -0.0155,  0.0198, -0.0244, -0.0623,  0.0951, -0.0742,  0.0839,\n",
      "        -0.0836,  0.0255,  0.0283, -0.0682,  0.0707,  0.0079,  0.0939, -0.0678,\n",
      "        -0.0370, -0.0143, -0.0780, -0.0020,  0.0217, -0.0424,  0.0840, -0.0057,\n",
      "         0.0963,  0.0866, -0.0352,  0.0033, -0.0441, -0.0470,  0.0436, -0.0422,\n",
      "        -0.0243,  0.0397,  0.0257, -0.0526,  0.0658,  0.0959, -0.0747,  0.0438,\n",
      "        -0.0202, -0.0637,  0.0788,  0.0435, -0.0187,  0.0363,  0.0017, -0.0450,\n",
      "         0.0854, -0.0599, -0.0482, -0.0539, -0.0795,  0.0680,  0.0207,  0.0633,\n",
      "        -0.0174, -0.0823, -0.0503, -0.0757,  0.0626, -0.0542, -0.0330,  0.0573,\n",
      "        -0.0733, -0.0451,  0.0369, -0.0075,  0.0638,  0.0634,  0.0107, -0.0680,\n",
      "         0.0835, -0.0323,  0.0951,  0.0455, -0.0929,  0.0406,  0.0026,  0.0027,\n",
      "        -0.0961,  0.0017,  0.0636,  0.0584, -0.0132,  0.0158,  0.0672,  0.0461,\n",
      "        -0.0326,  0.0028, -0.0431, -0.0021,  0.0313, -0.0872,  0.0507, -0.0374,\n",
      "         0.0316,  0.0987, -0.0124, -0.0421,  0.0716, -0.0953,  0.0159, -0.0665,\n",
      "         0.0973, -0.0905,  0.0628, -0.0324,  0.0055,  0.0138,  0.0385, -0.0589,\n",
      "        -0.0771,  0.0874, -0.0596, -0.0814,  0.0107,  0.0735,  0.0063, -0.0427,\n",
      "         0.0391,  0.0519, -0.0880,  0.0925,  0.0561, -0.0121,  0.0789, -0.0395,\n",
      "        -0.0785,  0.0301,  0.0849,  0.0325, -0.0932,  0.0807,  0.0987,  0.0482,\n",
      "         0.0862,  0.0604,  0.0629, -0.0709,  0.0255, -0.0148, -0.0454, -0.0840,\n",
      "        -0.0220, -0.0546,  0.0376, -0.0046, -0.0226,  0.0693, -0.0886,  0.0735,\n",
      "        -0.0039, -0.0847, -0.0930,  0.0677, -0.0742,  0.0353,  0.0746, -0.0764,\n",
      "         0.0547,  0.0242,  0.0987, -0.0929, -0.0041, -0.0410, -0.0103, -0.0832],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0326,  0.0149, -0.0354,  ...,  0.0035,  0.0149,  0.0240],\n",
      "        [-0.0272, -0.0384, -0.0290,  ..., -0.0698, -0.0360, -0.0236],\n",
      "        [-0.0317,  0.0162,  0.0455,  ...,  0.0361,  0.0596,  0.0504],\n",
      "        ...,\n",
      "        [-0.0154, -0.0548,  0.0567,  ..., -0.0349, -0.0552, -0.0514],\n",
      "        [ 0.0431, -0.0504, -0.0272,  ...,  0.0299, -0.0501, -0.0162],\n",
      "        [ 0.0502, -0.0702,  0.0448,  ..., -0.0017,  0.0250, -0.0352]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0096, -0.0559, -0.0071,  0.0216, -0.0327, -0.0141,  0.0007,  0.0020,\n",
      "         0.0413, -0.0097], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params: \n",
      "Parameter containing:\n",
      "tensor([[-0.0326,  0.0149, -0.0354,  ...,  0.0035,  0.0149,  0.0240],\n",
      "        [-0.0272, -0.0384, -0.0290,  ..., -0.0698, -0.0360, -0.0236],\n",
      "        [-0.0317,  0.0162,  0.0455,  ...,  0.0361,  0.0596,  0.0504],\n",
      "        ...,\n",
      "        [-0.0154, -0.0548,  0.0567,  ..., -0.0349, -0.0552, -0.0514],\n",
      "        [ 0.0431, -0.0504, -0.0272,  ...,  0.0299, -0.0501, -0.0162],\n",
      "        [ 0.0502, -0.0702,  0.0448,  ..., -0.0017,  0.0250, -0.0352]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0096, -0.0559, -0.0071,  0.0216, -0.0327, -0.0141,  0.0007,  0.0020,\n",
      "         0.0413, -0.0097], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The Model: ')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer: ')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel parameters: ')\n",
    "for param in  tinymodel.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('\\n\\nLayer params: ')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b098cbc",
   "metadata": {},
   "source": [
    "Common Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ba1e2",
   "metadata": {},
   "source": [
    "Most basic type of neural network is linear or fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b064964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tensor([[0.8771, 0.8899, 0.2731]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2607, -0.5673, -0.0860],\n",
      "        [ 0.4644, -0.3599,  0.1655]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0286, -0.3336], requires_grad=True)\n",
      "\n",
      "\n",
      "Output: \n",
      "tensor([[-0.3283, -0.2014]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input: ')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "    \n",
    "y = lin(x)\n",
    "print('\\n\\nOutput: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532b29e",
   "metadata": {},
   "source": [
    "If we multiply 'x' by the linear layer's weights, and add the biases, we will get the output 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89134f",
   "metadata": {},
   "source": [
    "Convolutional Layers(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44431695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black n white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)  # (no of input channels, no of output features, window/kernel size)\n",
    "        # output tensor to con1 gives us 6x28x28. 6 features with 28x28 height n width of map\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120) # 6 * 6 from image resolution\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is square, we can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except batch domension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "            \n",
    "        return num_features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3e2f3",
   "metadata": {},
   "source": [
    "Recurrent Layers (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd33cf",
   "metadata": {},
   "source": [
    "RNN are used for sequential data, from time series to the DNA nucleotides. An RNN does this by maintaining a hidden state that acts as a srt of memory for what is has been seen in hte sequence so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM takes work embeddings as inputs and outputs hidden states with dimensionality hidden_dim\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # the linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44252e4a",
   "metadata": {},
   "source": [
    "Constructor has four arguments:\n",
    "    1. vocab_size : no of words in input vocabulary. Each word is one-hot vector\n",
    "    2. tagset_size : no of tags in the output set\n",
    "    3. embedding_dim : size of embedding space for the vocabulary\n",
    "    4. hidden_dim : the size of the LSTM's memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbeef5",
   "metadata": {},
   "source": [
    "Data Manipulation Layers\n",
    "\n",
    "Max pooling reduce a tensor by combining cells, an dassinging the maximumvalue of the input cells to the output cell. This works as a layer that perform important function, but don't participate in larning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2209c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
