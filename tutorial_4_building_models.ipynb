{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e6c423",
   "metadata": {},
   "source": [
    "# Building Models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01fb4c",
   "metadata": {},
   "source": [
    "torch.nn.Module and torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6209aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model: \n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer: \n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model parameters: \n",
      "Parameter containing:\n",
      "tensor([[ 0.0228, -0.0951, -0.0068,  ...,  0.0135,  0.0041, -0.0894],\n",
      "        [-0.0156, -0.0455, -0.0296,  ...,  0.0958,  0.0287, -0.0145],\n",
      "        [-0.0017,  0.0066,  0.0915,  ...,  0.0712, -0.0852,  0.0636],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0421,  0.0352,  ..., -0.0230,  0.0389, -0.0771],\n",
      "        [-0.0827,  0.0512, -0.0924,  ...,  0.0151, -0.0472, -0.0377],\n",
      "        [ 0.0446,  0.0263, -0.0705,  ...,  0.0504, -0.0786, -0.0434]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 7.5023e-02, -8.9974e-02,  7.6222e-02, -3.6827e-02, -8.4915e-02,\n",
      "        -5.1428e-03, -5.5815e-03,  3.3337e-03,  1.4258e-02,  5.8927e-02,\n",
      "        -8.5550e-02,  5.2066e-02, -7.2367e-02,  6.9285e-02, -4.4030e-02,\n",
      "        -5.2571e-02,  1.0448e-02, -7.0976e-02, -4.5312e-02, -7.5576e-03,\n",
      "         7.7448e-02,  6.1325e-02,  9.5089e-02, -7.0869e-02, -7.9484e-02,\n",
      "        -4.8595e-02,  3.1433e-02, -5.8060e-02,  7.9300e-03, -3.7467e-02,\n",
      "         9.5229e-02,  9.3194e-03,  1.2608e-02,  2.2488e-02,  6.8440e-02,\n",
      "         5.4268e-02, -8.0766e-02,  7.6039e-02,  5.9391e-02, -5.0334e-02,\n",
      "        -3.1094e-02, -2.5140e-02, -2.1470e-02, -2.6506e-03,  1.1299e-02,\n",
      "         5.8829e-03, -2.2273e-02,  9.5607e-02, -3.9424e-03,  9.4502e-02,\n",
      "         4.5516e-02,  5.8803e-02, -3.4423e-02,  5.2661e-02,  4.4651e-02,\n",
      "         5.1099e-03, -6.8897e-02,  6.7874e-02, -6.2717e-02,  6.0772e-02,\n",
      "         8.2072e-02, -6.7239e-02,  4.0921e-02,  9.8991e-02, -7.7161e-02,\n",
      "         7.4834e-02,  8.7291e-02,  3.0460e-03, -4.6242e-02,  7.6268e-02,\n",
      "        -9.3943e-02, -4.7807e-02, -5.4341e-03,  8.7891e-02, -3.2628e-02,\n",
      "        -3.7431e-02,  3.4096e-02,  1.9544e-02, -2.2186e-02,  5.0544e-02,\n",
      "         4.3685e-02,  5.6249e-02,  3.8067e-02,  7.9043e-02, -2.3600e-02,\n",
      "        -7.6418e-02,  9.4829e-03, -9.5498e-02,  2.0555e-02,  8.6330e-02,\n",
      "        -2.6829e-02,  6.8110e-03,  5.3295e-02,  4.6337e-02,  9.1973e-02,\n",
      "         2.1791e-02,  7.8571e-02,  5.3049e-02,  1.1827e-03,  7.6151e-02,\n",
      "        -1.8145e-02,  8.9022e-02,  8.9502e-02, -2.0023e-02, -4.6908e-02,\n",
      "         6.7802e-02, -6.0901e-02, -3.2090e-02,  3.8975e-02, -4.3348e-02,\n",
      "        -9.0538e-02,  9.7941e-03, -8.0360e-02,  5.9647e-02,  9.0830e-02,\n",
      "         4.9785e-02,  5.9999e-02,  7.8088e-03, -1.4751e-02, -3.9278e-02,\n",
      "         5.3066e-03,  6.4295e-02, -3.5037e-02,  3.8732e-03,  8.4813e-03,\n",
      "        -9.9606e-02,  1.8600e-02, -9.3280e-02,  7.0406e-02,  7.9592e-03,\n",
      "         5.5399e-02,  6.7102e-02, -9.9720e-02, -1.1837e-02,  6.0838e-02,\n",
      "        -7.4127e-02, -7.3381e-03, -7.8757e-02, -2.6681e-02,  1.0825e-02,\n",
      "         3.7268e-02,  6.4205e-02,  3.8835e-02, -1.5560e-02, -9.0835e-02,\n",
      "        -2.7617e-03,  1.5662e-02, -6.2638e-02, -1.4014e-03,  8.6815e-02,\n",
      "        -2.7451e-02, -9.5763e-02,  9.4014e-02, -5.7412e-02, -8.7263e-02,\n",
      "         1.2088e-02, -3.7110e-02,  4.6149e-03, -3.2080e-02, -6.5345e-02,\n",
      "         4.4516e-02, -3.8877e-02, -2.2703e-02,  7.7856e-03,  1.7366e-02,\n",
      "         5.2185e-02, -3.9559e-02, -2.7292e-02, -6.5463e-03, -6.4675e-02,\n",
      "        -8.7625e-02, -1.2898e-02, -2.9595e-02, -4.6436e-02,  3.2835e-02,\n",
      "         3.1592e-03, -6.9033e-03, -3.8416e-02,  4.8933e-02, -9.5898e-02,\n",
      "         3.1642e-02, -7.1423e-02, -2.8961e-02,  2.5525e-02,  4.4466e-02,\n",
      "        -1.8496e-02,  8.6644e-02,  7.2957e-02,  3.3095e-02,  8.7186e-04,\n",
      "         4.6400e-02, -8.2774e-02, -1.0752e-02, -4.7450e-02,  1.1099e-02,\n",
      "        -8.4466e-02,  5.6952e-02, -5.5170e-05,  4.0526e-02,  7.2650e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0369, -0.0076, -0.0043,  ...,  0.0532, -0.0075, -0.0119],\n",
      "        [-0.0358, -0.0298, -0.0025,  ...,  0.0448, -0.0477, -0.0352],\n",
      "        [ 0.0140,  0.0702,  0.0086,  ...,  0.0487, -0.0428,  0.0461],\n",
      "        ...,\n",
      "        [ 0.0262,  0.0200, -0.0175,  ...,  0.0550, -0.0174, -0.0390],\n",
      "        [-0.0005, -0.0369,  0.0097,  ...,  0.0090,  0.0020,  0.0101],\n",
      "        [-0.0564, -0.0132,  0.0239,  ...,  0.0198, -0.0629, -0.0288]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0051, -0.0383, -0.0690, -0.0145,  0.0593, -0.0611,  0.0264, -0.0211,\n",
      "        -0.0213, -0.0051], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params: \n",
      "Parameter containing:\n",
      "tensor([[ 0.0369, -0.0076, -0.0043,  ...,  0.0532, -0.0075, -0.0119],\n",
      "        [-0.0358, -0.0298, -0.0025,  ...,  0.0448, -0.0477, -0.0352],\n",
      "        [ 0.0140,  0.0702,  0.0086,  ...,  0.0487, -0.0428,  0.0461],\n",
      "        ...,\n",
      "        [ 0.0262,  0.0200, -0.0175,  ...,  0.0550, -0.0174, -0.0390],\n",
      "        [-0.0005, -0.0369,  0.0097,  ...,  0.0090,  0.0020,  0.0101],\n",
      "        [-0.0564, -0.0132,  0.0239,  ...,  0.0198, -0.0629, -0.0288]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0051, -0.0383, -0.0690, -0.0145,  0.0593, -0.0611,  0.0264, -0.0211,\n",
      "        -0.0213, -0.0051], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The Model: ')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer: ')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel parameters: ')\n",
    "for param in  tinymodel.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('\\n\\nLayer params: ')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b098cbc",
   "metadata": {},
   "source": [
    "Common Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ba1e2",
   "metadata": {},
   "source": [
    "Most basic type of neural network is linear or fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b064964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tensor([[0.0746, 0.3789, 0.3715]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.0777, -0.4211, -0.5420],\n",
      "        [ 0.0687,  0.4271, -0.4527]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1407, -0.3300], requires_grad=True)\n",
      "\n",
      "\n",
      "Output: \n",
      "tensor([[-0.2260, -0.3313]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input: ')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "    \n",
    "y = lin(x)\n",
    "print('\\n\\nOutput: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532b29e",
   "metadata": {},
   "source": [
    "If we multiply 'x' by the linear layer's weights, and add the biases, we will get the output 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89134f",
   "metadata": {},
   "source": [
    "Convolutional Layers(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44431695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black n white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)  # (no of input channels, no of output features, window/kernel size)\n",
    "        # output tensor to con1 gives us 6x28x28. 6 features with 28x28 height n width of map\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120) # 6 * 6 from image resolution\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is square, we can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except batch domension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "            \n",
    "        return num_features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3e2f3",
   "metadata": {},
   "source": [
    "Recurrent Layers (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd33cf",
   "metadata": {},
   "source": [
    "RNN are used for sequential data, from time series to the DNA nucleotides. An RNN does this by maintaining a hidden state that acts as a srt of memory for what is has been seen in hte sequence so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM takes work embeddings as inputs and outputs hidden states with dimensionality hidden_dim\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # the linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44252e4a",
   "metadata": {},
   "source": [
    "Constructor has four arguments:\n",
    "    1. vocab_size : no of words in input vocabulary. Each word is one-hot vector\n",
    "    2. tagset_size : no of tags in the output set\n",
    "    3. embedding_dim : size of embedding space for the vocabulary\n",
    "    4. hidden_dim : the size of the LSTM's memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145821fc",
   "metadata": {},
   "source": [
    "Data Manipulation Layers\n",
    "\n",
    "Max pooling reduce a tensor by combining cells, an dassinging the maximumvalue of the input cells to the output cell. This works as a layer that perform important function, but don't participate in larning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc42306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4453, 0.1965, 0.2766, 0.4354, 0.4527, 0.4815],\n",
      "         [0.0483, 0.6444, 0.4422, 0.0401, 0.0244, 0.8276],\n",
      "         [0.0756, 0.5548, 0.3653, 0.3061, 0.4284, 0.8537],\n",
      "         [0.2576, 0.9806, 0.3392, 0.6383, 0.0998, 0.2208],\n",
      "         [0.6323, 0.5258, 0.1660, 0.8183, 0.0055, 0.2510],\n",
      "         [0.7725, 0.0396, 0.5836, 0.8612, 0.5755, 0.1271]]])\n",
      "tensor([[[0.6444, 0.8537],\n",
      "         [0.9806, 0.8612]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea38002",
   "metadata": {},
   "source": [
    "Normalization layer re-center and normalize the output of one layer before feeding it to another. Centering and scaling the intermediate tensors has a number of beneficial effects, such as letting use higher learning rates without exploding/vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244c831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8.9663, 18.1267, 23.2359, 19.7406],\n",
      "         [13.7878, 13.1657, 16.5746,  9.4849],\n",
      "         [18.8747, 14.9917, 10.2237,  9.2706],\n",
      "         [18.0279, 10.0434, 19.5325, 13.1947]]])\n",
      "tensor(14.8276)\n",
      "tensor([[[-1.6223,  0.1156,  1.0849,  0.4218],\n",
      "         [ 0.2116, -0.0346,  1.3148, -1.4918],\n",
      "         [ 1.4334,  0.4277, -0.8072, -1.0540],\n",
      "         [ 0.7467, -1.3614,  1.1440, -0.5294]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-5.9605e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc50727",
   "metadata": {},
   "source": [
    "Dropout layers are tools for encouraging sparse representations in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e4a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6438, 0.7842, 0.3329, 0.7233],\n",
      "         [0.0000, 0.7365, 0.1289, 0.9480],\n",
      "         [1.2457, 0.7169, 0.1661, 1.2080],\n",
      "         [1.0845, 0.0000, 1.0175, 0.4237]]])\n",
      "tensor([[[0.6438, 0.7842, 0.3329, 0.0000],\n",
      "         [0.0000, 0.7365, 0.1289, 0.9480],\n",
      "         [1.2457, 0.7169, 0.1661, 0.0000],\n",
      "         [1.0845, 0.0000, 0.0000, 0.4237]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.2)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cc018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
